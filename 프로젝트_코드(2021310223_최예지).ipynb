{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7yCIl5TYvaH"
      },
      "source": [
        "자동차 브랜드 정보 제공 검색 서비스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wsqlDCYrMD"
      },
      "source": [
        "라이브러리 import, install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB0JkuUCXsm-",
        "outputId": "7e49ef40-fb5c-42fc-a018-79427cc5fe50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#준비\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UOJ1pHOX_v4",
        "outputId": "29b54af2-5f04-487e-8719-269ed365e2f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Wa\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Wa\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r0% [4 InRelease 12.7 kB/114 kB 11%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\r0% [4 InRelease 15.6 kB/114 kB 14%] [Connecting to security.ubuntu.com (91.189.\r                                                                               \rHit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "\r0% [4 InRelease 69.2 kB/114 kB 61%] [Connecting to security.ubuntu.com (91.189.\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rGet:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "\r0% [8 InRelease 6,944 B/108 kB 6%] [Waiting for headers] [Waiting for headers]\r                                                                              \rHit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "\r0% [8 InRelease 28.7 kB/108 kB 26%] [Waiting for headers] [Connecting to ppa.la\r                                                                               \r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r                                                                           \rHit:10 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Fetched 340 kB in 1s (272 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n",
            "g++ is already the newest version (4:9.3.0-1ubuntu2).\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "python-dev-is-python2 is already the newest version (2.7.17-4).\n",
            "openjdk-8-jdk is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: JPype1-py3 in /usr/local/lib/python3.10/dist-packages (0.5.5.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 설치\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "!pip3 install JPype1-py3\n",
        "!pip3 install konlpy\n",
        "# JAVA_HOME 환경변수 설정\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATzgtdqlYP02",
        "outputId": "c2f0b85b-1a6e-4a97-b50c-47118930003f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt') # tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLCJP0mWz-FZ"
      },
      "source": [
        "데이터 수집"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV052dQdY2Gg"
      },
      "source": [
        "Naver Cafe 검색 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSORA-RrYTjy"
      },
      "outputs": [],
      "source": [
        "#사용자가 입력한 검색 query로 카페글 검색 API를 불러오는 함수 정의 -> 검색한 정보에 대한 검색 결과 데이터 프레임 반환\n",
        "def naver_cafe(input_word):\n",
        "  client_id = \"Bwq4GJAba1UhQUSG9RDd\"\n",
        "  client_secret = \"0qM9a1vSPt\"\n",
        "\n",
        "  query = urllib.parse.quote(input_word)\n",
        "\n",
        "  idx = 0       # 여러개 정보 받을 때 횟수 카운트 용 \n",
        "  display = 10 # 10개 단위로 정보 받아오기\n",
        "  start = 1     # 첫 시작점\n",
        "  end = 1000    # 마지막 시작점\n",
        "\n",
        "  cafe_df = pd.DataFrame(columns=('Title', 'Link', 'Description', 'Cafe Name', 'Cafe URL'))\n",
        "\n",
        "  for start_index in range(start, end, display):\n",
        "\n",
        "    url = \"https://openapi.naver.com/v1/search/cafearticle?query=\" + query \\\n",
        "          + '&display=' + str(display) \\\n",
        "          + '&start=' + str(start_index)\n",
        "\n",
        "    request = urllib.request.Request(url)\n",
        "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "    response = urllib.request.urlopen(request)\n",
        "    rescode = response.getcode()\n",
        "    if(rescode==200): # API에 정상적인 응답을 받은 경우(데이터 크롤링에 성공한 경우)\n",
        "        response_body = response.read()\n",
        "        response_dict = json.loads(response_body.decode('utf-8'))\n",
        "        items = response_dict['items'] # 원하는 정보가 담겨있는 dict\n",
        "        for item_index in range(0, len(items)):\n",
        "          remove_tag = re.compile('<.*?>')\n",
        "          \n",
        "          title = re.sub(remove_tag, '', items[item_index]['title'])\n",
        "          link = items[item_index]['link']\n",
        "          description = re.sub(remove_tag, '', items[item_index]['description'])\n",
        "          cafename = items[item_index]['cafename']\n",
        "          cafeurl = items[item_index]['cafeurl']\n",
        "\n",
        "          cafe_df.loc[idx] = [title, link, description, cafename, cafeurl]\n",
        "          idx += 1\n",
        "    else:\n",
        "        print(\"Error Code:\" + rescode)\n",
        "\n",
        "  return cafe_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h02A9Q4Y-mO"
      },
      "source": [
        "Naver News 검색 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB05JmlgY9wl"
      },
      "outputs": [],
      "source": [
        "#사용자가 입력한 검색 query로 뉴스 검색 API를 불러오는 함수 정의 -> 검색한 정보에 대한 검색 결과 데이터 프레임 반환\n",
        "def naver_news(input_word):\n",
        "  client_id = \"Bwq4GJAba1UhQUSG9RDd\"\n",
        "  client_secret = \"0qM9a1vSPt\"\n",
        "\n",
        "  query = urllib.parse.quote(input_word)\n",
        "\n",
        "  idx = 0       # 여러개 정보 받을 때 횟수 카운트 용 \n",
        "  display = 10 # 10개 단위로 정보 받아오기\n",
        "  start = 1     # 첫 시작점\n",
        "  end = 1000    # 마지막 시작점\n",
        "\n",
        "  news_df = pd.DataFrame(columns=('Title', 'Link', 'desc', 'date'))\n",
        "\n",
        "  for start_index in range(start, end, display):\n",
        "\n",
        "    url = \"https://openapi.naver.com/v1/search/news?query=\" + query \\\n",
        "          + '&display=' + str(display) \\\n",
        "          + '&start=' + str(start_index)\n",
        "\n",
        "    request = urllib.request.Request(url)\n",
        "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "    response = urllib.request.urlopen(request)\n",
        "    rescode = response.getcode()\n",
        "    if(rescode==200): # API에 정상적인 응답을 받은 경우(데이터 크롤링에 성공한 경우)\n",
        "        response_body = response.read()\n",
        "        response_dict = json.loads(response_body.decode('utf-8'))\n",
        "        items = response_dict['items'] # 원하는 정보가 담겨있는 dict\n",
        "        for item_index in range(0, len(items)):\n",
        "          #수정할 부분\n",
        "          remove_tag = re.compile('<.*?>')\n",
        "          title = re.sub(remove_tag, '', items[item_index]['title'])\n",
        "          link = items[item_index]['link']\n",
        "          org_link = items[item_index]['originallink']\n",
        "          desc = items[item_index]['description']\n",
        "          date = items[item_index]['pubDate']\n",
        "\n",
        "          news_df.loc[idx] = [title, link, desc, date]\n",
        "          idx += 1\n",
        "    else:\n",
        "        print(\"Error Code:\" + rescode)\n",
        "\n",
        "  return news_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kagXzqljZMBG"
      },
      "source": [
        "Naver Blog 검색 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3cGymMRZO8l"
      },
      "outputs": [],
      "source": [
        "#사용자가 입력한 검색 query로 블로그 검색 API를 불러오는 함수 정의 -> 검색한 정보에 대한 검색 결과 데이터 프레임 반환\n",
        "def naver_blog(input_word):\n",
        "  client_id = \"Bwq4GJAba1UhQUSG9RDd\"\n",
        "  client_secret = \"0qM9a1vSPt\"\n",
        "\n",
        "  query = urllib.parse.quote(input_word)\n",
        "\n",
        "  idx = 0       # 여러개 정보 받을 때 횟수 카운트 용 \n",
        "  display = 10 # 10개 단위로 정보 받아오기\n",
        "  start = 1     # 첫 시작점\n",
        "  end = 1000    # 마지막 시작점\n",
        "\n",
        "  blog_df = pd.DataFrame(columns=('Title', 'Link', 'Description', 'Blogger Name', 'Blogger Link'))\n",
        "\n",
        "  for start_index in range(start, end, display):\n",
        "\n",
        "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + query \\\n",
        "          + '&display=' + str(display) \\\n",
        "          + '&start=' + str(start_index)\n",
        "\n",
        "    request = urllib.request.Request(url)\n",
        "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "    response = urllib.request.urlopen(request)\n",
        "    rescode = response.getcode()\n",
        "    if(rescode==200): # API에 정상적인 응답을 받은 경우(데이터 크롤링에 성공한 경우)\n",
        "        response_body = response.read()\n",
        "        response_dict = json.loads(response_body.decode('utf-8'))\n",
        "        items = response_dict['items'] # 원하는 정보가 담겨있는 dict\n",
        "        for item_index in range(0, len(items)):\n",
        "          remove_tag = re.compile('<.*?>')\n",
        "          \n",
        "          title = re.sub(remove_tag, '', items[item_index]['title'])\n",
        "          link = items[item_index]['link']\n",
        "          description = re.sub(remove_tag, '', items[item_index]['description'])\n",
        "          blogger_name = items[item_index]['bloggername']\n",
        "          blogger_link = items[item_index]['bloggerlink']\n",
        "\n",
        "          blog_df.loc[idx] = [title, link, description, blogger_name, blogger_link]\n",
        "          idx += 1\n",
        "    else:\n",
        "        print(\"Error Code:\" + rescode)\n",
        "  \n",
        "  return blog_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txR_hnlsZaih"
      },
      "source": [
        "카페, 뉴스, 블로그 내용 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU3I99XaZZqb"
      },
      "outputs": [],
      "source": [
        "#한국어 텍스트 전처리 함수 정의\n",
        "def clean_korean_text(raw_text):\n",
        "  #불용어 리스트 불러오기\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/stopword_car.txt', 'r', encoding='utf8') as f:\n",
        "    stopwords_car = f.read().splitlines()\n",
        "  \n",
        "  #불용어 제거한 리스트 저장할 리스트 생성\n",
        "  removed_stopwords_l = []\n",
        "  #저장된 리스트 안의 문장 단위 탐색\n",
        "  for sent in raw_text:\n",
        "    #형태소 단위로 분석\n",
        "    for_stopwords_list = okt.morphs(sent)\n",
        "    #불용어 제거하여 저장할 리스트 생성\n",
        "    word_non_stop = []\n",
        "    #문장 안의 하나의 단어에 대해서\n",
        "    for word in for_stopwords_list:\n",
        "      #불용어 목록에 없다면\n",
        "      if word not in stopwords_car:\n",
        "        #제거한 리스트에 저장\n",
        "        word_non_stop.append(word)\n",
        "    #중첩 리스트 형태로 문장 단위로 저장\n",
        "    removed_stopwords_l.append(word_non_stop)\n",
        "\n",
        "  #형태소 분석\n",
        "  #형태소 분석하여 저장할 리스트 생성\n",
        "  clean_words = []\n",
        "  #각 문장에 대해서\n",
        "  for text in removed_stopwords_l:\n",
        "    #pos tagging\n",
        "    tagged_words = okt.pos(' '.join(text))\n",
        "    #명사, 동사, 형용사에 해당하는 단어만 리스트에 저장 \n",
        "    selected_words = [word for word, pos in tagged_words if pos in ['Noun', 'Verb', 'Adjective']]\n",
        "    #중첩 리스트 형태로 문장 단위 저장\n",
        "    clean_words.append(selected_words)\n",
        "  \n",
        "  return clean_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUcZbNyGI5hc"
      },
      "outputs": [],
      "source": [
        "#카페 데이터 프레임 -> 리스트로 저장, 전처리\n",
        "def cafe_process_api(cafe_df):\n",
        "  #dataframe에서 텍스트 칼럼만 추출\n",
        "  cafe_df_drop = cafe_df.drop(['Link', 'Cafe Name', 'Cafe URL'], axis = 1) \n",
        "  #텍스트 저장을 위한 빈 리스트 생성\n",
        "  cafe_texts = []\n",
        "\n",
        "  for column in ['Title', 'Description']:\n",
        "    #토큰화, 어간 추출 및 원형 복원\n",
        "    cafe_df_drop[column] = cafe_df_drop[column].apply(lambda x: ' '.join(okt.morphs(x)))\n",
        "    #하나의 문자열로 텍스트 저장\n",
        "    cafe_texts += cafe_df_drop[column].tolist()\n",
        "\n",
        "  #전처리\n",
        "  cafe_cleaned_text = clean_korean_text(cafe_texts)\n",
        "\n",
        "  return cafe_cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1JQN0v4KTyb"
      },
      "outputs": [],
      "source": [
        "#뉴스 데이터 프레임 -> 리스트로 저장, 전처리\n",
        "def news_process_api(news_df):\n",
        "  #dataframe에서 텍스트 칼럼만 추출\n",
        "  news_df_drop = news_df.drop(['Link', 'date'], axis = 1) \n",
        "  #텍스트 저장을 위한 빈 리스트 생성\n",
        "  news_texts = []\n",
        "\n",
        "  for column in ['Title', 'desc']:\n",
        "    #토큰화, 어간 추출 및 원형 복원\n",
        "    news_df_drop[column] = news_df_drop[column].apply(lambda x: ' '.join(okt.morphs(x)))\n",
        "    #하나의 문자열로 텍스트 저장\n",
        "    news_texts += news_df_drop[column].tolist()\n",
        "\n",
        "  #전처리\n",
        "  news_cleaned_text = clean_korean_text(news_texts)\n",
        "\n",
        "  return news_cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYRcq7e6LR2R"
      },
      "outputs": [],
      "source": [
        "#블로그 데이터 프레임 -> 리스트로 저장, 전처리\n",
        "def blog_process_api(blog_df):\n",
        "  #dataframe에서 텍스트 칼럼만 추출\n",
        "  blog_df_drop = blog_df.drop(['Link', 'Blogger Name', 'Blogger Link'], axis = 1) \n",
        "  #텍스트 저장을 위한 빈 리스트 생성\n",
        "  blog_texts = []\n",
        "\n",
        "  for column in ['Title', 'Description']:\n",
        "    #토큰화, 어간 추출 및 원형 복원\n",
        "    blog_df_drop[column] = blog_df_drop[column].apply(lambda x: ' '.join(okt.morphs(x)))\n",
        "    #하나의 문자열로 텍스트 저장\n",
        "    blog_texts += blog_df_drop[column].tolist()\n",
        "\n",
        "  #전처리\n",
        "  blog_cleaned_text = clean_korean_text(blog_texts)\n",
        "\n",
        "  return blog_cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jsVwIGn0B0R"
      },
      "source": [
        "데이터 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwaGUNPZ0D-g"
      },
      "outputs": [],
      "source": [
        "#상위 빈도수의 단어와 빈도수 추출하는 함수\n",
        "def word_count(df_text, brand_name, search_word):\n",
        "  #결과에서 검색어를 제외하기 위해 검색어를 문자열, 리트스로 저장\n",
        "  input_word = brand_name + search_word\n",
        "  remove_words = [brand_name, search_word]\n",
        "\n",
        "  #단어 빈도를 저장할 counter 객체 생성\n",
        "  word_counts = Counter()\n",
        "\n",
        "  #모든 문장에 대해\n",
        "  for sent in df_text:\n",
        "    '''#명사 추출\n",
        "    # words = [word for word, pos in sent if pos in ['Noun', 'Adjective']]'''\n",
        "    #검색어 제외하고 단어 리스트에 저장\n",
        "    for_count_words = [word for word in sent if word not in remove_words]\n",
        "    \n",
        "    #단어 빈도 계산 (검색어와 한 글자 단어는 제외) - 단어와 빈도수를 계속 for문 돌면서 추가(update)\n",
        "    word_counts.update(word for word in for_count_words if word != input_word and len(word) > 1)\n",
        "\n",
        "  #가장 많이 등장하는 단어 7개만 출력\n",
        "  #상위 단어 저장할 리스트 생성\n",
        "  most_word = [] \n",
        "  for word, count in word_counts.most_common(7):\n",
        "    #단어와 빈도수를 튜플 형태로 저장\n",
        "    most_word_tuple = (word, count) \n",
        "    most_word.append(most_word_tuple)\n",
        "\n",
        "  #최종 결과로 상위 7개 단어의 단어와 빈도수가 저장된 리스트 반환\n",
        "  return most_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArpyM9641dt9"
      },
      "outputs": [],
      "source": [
        "#감성 분석 \n",
        "#파일 로드 함수\n",
        "def load_files(filepath):\n",
        "  with open(filepath, 'r', encoding = 'utf-8') as file:\n",
        "    words = file.read().splitlines()\n",
        "  return words\n",
        "\n",
        "#위 함수로 감성사전 불러오기\n",
        "pos_words = load_files('/content/drive/MyDrive/Colab Notebooks/knu_pos.txt')\n",
        "neg_words = load_files('/content/drive/MyDrive/Colab Notebooks/knu_neg.txt')\n",
        "unknown_words = load_files('/content/drive/MyDrive/Colab Notebooks/knu_unknown.txt')\n",
        "\n",
        "#감성 분석 함수 - 긍정, 부정, 중립어 수 세기 -> 반환\n",
        "def emotion_analysis(input_text):\n",
        "  positive_count = 0 #긍정어 수\n",
        "  negative_count = 0 #부정어 수\n",
        "  neutral_count = 0 #중립어 수\n",
        "  total_count = 0 #전체 단어수\n",
        "\n",
        "  #단어 개수 세기 -> count에 추가\n",
        "  for e_text in input_text:\n",
        "    total_count += len(e_text)\n",
        "    for word in e_text:\n",
        "      if word in pos_words:\n",
        "        positive_count += 1\n",
        "      elif word in neg_words:\n",
        "        negative_count += 1\n",
        "      elif word in unknown_words:\n",
        "        neutral_count += 1\n",
        "\n",
        "  return total_count, positive_count, negative_count, neutral_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEA8qy9N4Q7W"
      },
      "outputs": [],
      "source": [
        "#랜덤으로 문장 3개 출력하는 함수\n",
        "def random_sent(org_df):\n",
        "  random_sents = []\n",
        "  random_rows = org_df['Description'].sample(n=3)\n",
        "  for row in random_rows:\n",
        "    random_sents.append(row)\n",
        "  return random_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICGTVXCwLDu3"
      },
      "outputs": [],
      "source": [
        "#검색어 주변 상위 빈도수의 단어 추출 함수\n",
        "def near_words(sent_list, brand_name):\n",
        "  #단어 빈도수 계산 함수와 유사\n",
        "  near_words = []\n",
        "  for sent in sent_list:\n",
        "    #다시 pos tagging\n",
        "    words_pos = okt.pos(' '.join(sent))\n",
        "    #명사와 형용사만 추출(동사 제외)\n",
        "    words = [word for word, pos in words_pos if pos in ['Noun', 'Adjective']]\n",
        "\n",
        "    if brand_name in words:\n",
        "      #문장의 단어 리스트에서 검색어의 index를 찾음\n",
        "      search_word_index = words.index(brand_name)\n",
        "      #검색어 index 양쪽으로 5개의 단어만 새 리스트에 저장\n",
        "      near_words += words[max(0, search_word_index-5):search_word_index]\n",
        "      near_words += words[search_word_index+1:min(len(words), search_word_index+6)]\n",
        "  #counter 객체 생성\n",
        "  word_counts = Counter()\n",
        "  #near_words에서 단어 빈도수 계산\n",
        "  word_counts.update(word for word in near_words if word != brand_name)\n",
        "  #가장 상위의 단어 4개 따로 저장\n",
        "  near_count_list = word_counts.most_common(4)\n",
        "\n",
        "  return near_count_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzKvZl3DPSiY"
      },
      "source": [
        "데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ts_w-k8f4t"
      },
      "outputs": [],
      "source": [
        "#브랜드 정보 검색 함수\n",
        "def brand_inform(brand_name, search_word):\n",
        "  #검색어 만들기\n",
        "  search_key = brand_name + ' ' + search_word\n",
        "  print()\n",
        "  print('검색어:', search_key)\n",
        "\n",
        "  #네이버 카페 데이터 처리\n",
        "  i_cafe_df = naver_cafe(search_key) #검색 결과 dataframe으로 저장\n",
        "  i_cafe_text = cafe_process_api(i_cafe_df) #검색 결과 전처리, 리스트로 저장\n",
        "  i_cafe_most_word = word_count(i_cafe_text, brand_name, search_word) #빈도수 count -> 결과는 리스트[(단어, 빈도수)]\n",
        "  cafe_near_count = near_words(i_cafe_text, brand_name) #brand name 주변 상위 빈도수 단어 count -> 결과는 리스트[(단어, 빈도수)]\n",
        "\n",
        "  #결과 출력 부분\n",
        "  print()\n",
        "  print('카페글 분석 결과')\n",
        "  print('가장 많이 등장하는 단어 7개: \\n1위-', *i_cafe_most_word[0], '\\n2위-', *i_cafe_most_word[1], '\\n3위-', *i_cafe_most_word[2], '\\n4위-', *i_cafe_most_word[3], '\\n5위-', *i_cafe_most_word[4], '\\n6위-', *i_cafe_most_word[5], '\\n7위-', *i_cafe_most_word[6])\n",
        "\n",
        "  print()\n",
        "  print('검색 결과 중 브랜드명과 함께 검색된 상위의 단어입니다.')\n",
        "  print(*cafe_near_count[0], sep=', ')\n",
        "  print(*cafe_near_count[1], sep=', ')\n",
        "  print(*cafe_near_count[2], sep=', ')\n",
        "\n",
        "  print()\n",
        "  print('-'*80)\n",
        "\n",
        "  #네이버 뉴스 데이터 처리\n",
        "  i_news_df = naver_news(search_key)\n",
        "  i_news_text = news_process_api(i_news_df)\n",
        "  i_news_most_word = word_count(i_news_text, brand_name, search_word)\n",
        "  news_near_count = near_words(i_news_text, brand_name)\n",
        "\n",
        "  #결과 출력 부분\n",
        "  print()\n",
        "  print('뉴스글 분석 결과')\n",
        "  print('가장 많이 등장하는 단어 7개: \\n1위-', *i_news_most_word[0], '\\n2위-', *i_news_most_word[1], '\\n3위-', *i_news_most_word[2], '\\n4위-', *i_news_most_word[3], '\\n5위-', *i_news_most_word[4], '\\n6위-', *i_news_most_word[5], '\\n7위-', *i_news_most_word[6])\n",
        "\n",
        "  print()\n",
        "  print(\"검색어와 가장 많이 같이 등장한 단어입니다. \")\n",
        "  print(*news_near_count[0], sep=', ')\n",
        "  print(*news_near_count[1], sep=', ')\n",
        "  print(*news_near_count[2], sep=', ')\n",
        "  \n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHUzqgdDTNER"
      },
      "outputs": [],
      "source": [
        "def review(brand_name, review_str):\n",
        "  #검색어 만들기 브랜드명+후기\n",
        "  search_word = brand_name + ' ' + review_str\n",
        "  print()\n",
        "  print('검색어:', search_word)\n",
        "\n",
        "  #네이버 카페 데이터 처리\n",
        "  r_cafe_df = naver_cafe(search_word) #검색 결과 dataframe으로 저장\n",
        "  r_cafe_text = cafe_process_api(r_cafe_df) #검색 결과 전처리, 리스트로 저장\n",
        "  r_cafe_most_word = word_count(r_cafe_text, brand_name, review_str) #빈도수 count -> 결과는 리스트[(단어, 빈도수)]\n",
        "  cafe_total_word, cafe_pos_word, cafe_neg_word, cafe_neu_word = emotion_analysis(r_cafe_text) #감성 분석 -> 전체 단어 개수, 긍정어, 부정어, 중립어 개수 변수에 저장\n",
        "  cafe_random_sents = random_sent(r_cafe_df) #random row  리스트로 저장\n",
        "\n",
        "  #결과 출력 부분\n",
        "  print()\n",
        "  print('후기 - 카페글 분석 결과')\n",
        "  print('가장 많이 등장하는 단어 7개: \\n1위-', *r_cafe_most_word[0], '\\n2위-', *r_cafe_most_word[1], '\\n3위-', *r_cafe_most_word[2], '\\n4위-', *r_cafe_most_word[3], '\\n5위-', *r_cafe_most_word[4], '\\n6위-', *r_cafe_most_word[5], '\\n7위-', *r_cafe_most_word[6])\n",
        "\n",
        "  print()\n",
        "  print(f'총 단어 개수: {cafe_total_word}')\n",
        "  print(f'긍정어 개수: {cafe_pos_word}')\n",
        "  print(f'부정어 개수: {cafe_neg_word}')\n",
        "  print(f'중립어 개수: {cafe_neu_word}')\n",
        "\n",
        "  print()\n",
        "  print('검색 결과 예시')\n",
        "  for row in cafe_random_sents:\n",
        "    print(row)\n",
        "  \n",
        "  print()\n",
        "  print('-'*80)\n",
        "\n",
        "  #네이버 블로그 데이터 처리\n",
        "  r_blog_df = naver_blog(search_word)\n",
        "  r_blog_text = blog_process_api(r_blog_df)\n",
        "  r_blog_most_word = word_count(r_blog_text, brand_name, review_str)\n",
        "  blog_total_word, blog_pos_word, blog_neg_word, blog_neu_word = emotion_analysis(r_blog_text)\n",
        "  blog_random_sents = random_sent(r_blog_df)\n",
        "\n",
        "  #결과 출력 부분\n",
        "  print()\n",
        "  print('후기 - 블로그글 분석 결과')\n",
        "  print('가장 많이 등장하는 단어 7개: \\n1위-', *r_blog_most_word[0], '\\n2위-', *r_blog_most_word[1], '\\n3위-', *r_blog_most_word[2], '\\n4위-', *r_blog_most_word[3], '\\n5위-', *r_blog_most_word[4], '\\n6위-', *r_blog_most_word[5], '\\n7위-', *r_blog_most_word[6])\n",
        "\n",
        "  print()\n",
        "  print(f'총 단어 개수: {blog_total_word}')\n",
        "  print(f'긍정어 개수: {blog_pos_word}')\n",
        "  print(f'부정어 개수: {blog_neg_word}')\n",
        "  print(f'중립어 개수: {blog_neu_word}')\n",
        "\n",
        "  print()\n",
        "  print('검색 결과 예시')\n",
        "  for row in blog_random_sents:\n",
        "    print(row)\n",
        "  \n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54vtRMm1W8BL"
      },
      "source": [
        "Main 서비스 (사용자 input, output 코드)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK65vJglW6Az",
        "outputId": "1e5253d1-89b1-4441-e863-77d26f2b10ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕하세요! \n",
            "자동차에 대한 정보를 주는 서비스입니다.\n",
            "\n",
            "\n",
            "\n",
            "검색어: 테슬라  \n",
            "\n",
            "카페글 분석 결과\n",
            "가장 많이 등장하는 단어 7개: \n",
            "1위- 모델 872 \n",
            "2위- 판매 496 \n",
            "3위- 가격 389 \n",
            "4위- 전기차 291 \n",
            "5위- 차량 199 \n",
            "6위- 거래 174 \n",
            "7위- 개인 170\n",
            "\n",
            "검색 결과 중 브랜드명과 함께 검색된 상위의 단어입니다.\n",
            "모델, 600\n",
            "전기차, 177\n",
            "가격, 170\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "뉴스글 분석 결과\n",
            "가장 많이 등장하는 단어 7개: \n",
            "1위- 주가 536 \n",
            "2위- 전기차 489 \n",
            "3위- 미국 466 \n",
            "4위- 상승 458 \n",
            "5위- 했다 339 \n",
            "6위- 모델 239 \n",
            "7위- 중국 237\n",
            "\n",
            "검색어와 가장 많이 같이 등장한 단어입니다. \n",
            "주가, 279\n",
            "전기차, 250\n",
            "미국, 209\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#사용자 입출력 cell\n",
        "\n",
        "print('안녕하세요! \\n자동차에 대한 정보를 주는 서비스입니다.')\n",
        "print()\n",
        "\n",
        "#프로그램 종료 전까지 계속 반복해서 돌아감\n",
        "while True: \n",
        "  #정보와 후기 중 궁금한 것 입력받기\n",
        "  category = input(\"무엇이 궁금하신가요? (정보 / 후기) - 종료 시 '종료' 입력: \")\n",
        "  print()\n",
        "\n",
        "  if category == '정보':\n",
        "    #검색할 자동차 브랜드 입력받기\n",
        "    brand_name = input('궁금한 자동차 브랜드를 입력해 주세요. : ')\n",
        "    #사용자로부터 검색할 검색어 입력받기\n",
        "    search_word = input('어떤 정보에 대해 검색하시나요? (그냥 브랜드만 궁금하다면 space를 한 번 눌러주세요): ')\n",
        "    #브랜드 정보 검색어를 입력하여 처리하는 함수 불러오기\n",
        "    brand_inform(brand_name, search_word)\n",
        "\n",
        "  elif category == '후기':\n",
        "    #검색할 자동차 브랜드 입력받기\n",
        "    brand_name = input('궁금한 자동차 브랜드를 입력해 주세요. : ')\n",
        "    review_str = '후기'\n",
        "    #후기 검색어 입력하여 처리하는 함수\n",
        "    review(brand_name, review_str)\n",
        "\n",
        "  #종료할 시에는 종료 입력\n",
        "  elif category == '종료':\n",
        "    print('이용해 주셔서 감사합니다.')\n",
        "    break\n",
        "  \n",
        "  else: \n",
        "    print('다시 입력해 주세요.')\n",
        "    print()\n",
        "\n",
        "  \n",
        "  print('='*80)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlCToJ8Ofgbv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}